# Step 1 - Sentence Transformer Model

## Overview
I use BERT backbone to implement a sentent transformer model. Please see below for my reasoning of model architecture choice.

## Model Architecture Discussion
### 1. **Projection Layer**
I apply a linear projection layer to reduce dimention to 256 from the raw 768 for embedding data product. Though this is optional, it is usually recommended to compress big-embedding-representations into smaller dim in industry, in order to reduce computation overhead in downtream application (both training and online sesrving) while also retaining majority of sentence meaning.

### 2. **Pooling**
I use `[CLS]` token emebdding to represent sentence level meaning. Alternative solution could also be mean pooling, for instance. I choose `[CLS]` as conventionally it is able to capture sentence level semantic meaning pretty well, while mean pooling could loss some information. In reality, the choice needs to tested & made w.r.t specific downstream tasks. 

### 3. **Enbaling GPU**
Usually by default I will add GPU support there if it is available.

### 4. **Visualization**
There are many ways to showcase sentence embedding and quality check its presentation's effectiveness. Dimension reduction then visualization is pretty common to start with. For simplicity I use PCA here to visualize sentence embeddings from 2D. Other choices can be made like via t-SNE, cosine similaity heatmap, etc.

## Test
Run `python step_1_sentence_transformer.py` script to generate embeddings and visualize.

---

# Step 2 - Multi task learner

# Multi-Task Learning with MoE

## Overview
I choose to implment a MoE based multi task learner for below NLP tasks:

1. **Sentence Categorization**: Classify sentence into one of four categories (0-Sports, 1-Technology, 2-Politics, 3-Food).
2. **Sentiment Analysis**: Determine whether a sentence is a 0-positive or 1-negative sentiment.

## Model Architecture Discussion

### 1. **MoE for Multi-Task Learning**
Instead of a fully shared model structure, I use MoE here:
- Multiple specialized expert networks (MLP) to process sentence embeddings.
- Each task has its specific gating network to dynamically extract most relevant information across experts for that task.
- Within MoE the model deployers a 2-layer expert transformation (768 -> 512 -> 256) for better computation efficiency. Projected raw-embedding into lower dimension is usually recommended for production-grade system to balance between model complexity and serving efficiency. The exact layer numbers and hidden dim are hyperparam.
- Finally, each task should have its specific classification head.


### 2. Loss function design for multi task learner
For simplicity I just combine 2 tasks's CE loss together without any weighting. In reality, usually multi objectives are weighted, considering each objective's importance, each task's train data size, etc. 

## Test
Run `python step_2_classification_sentiment.py` script to initiate model training and test case. Note that all sample sentences are generated by ChatGPT.

---

# Step 3 - Discussion question

## 3.1: discuss how to decide which portions of the network to train and which parts to keep frozen.

1. when to freeze backbone and only train task specific layers?
- task specific data set is small
- for computation efficiency or trying to reduce training time

2. when to train the backbone:
- dataset is lartge enough, allowin model to dig deep & learn domain specific nuance
- task involves very domain-specific language (like medical or legal document) where the pre-trained backbone might not generalize well

3. when to freeze one head while training others:
- it could be the case that one task's data distribution is very different from all other tasks that might require further adaptation
- Or if I want to fine tune only 1 task specifically while trying to keep other heads fixed.


## 3.2 When to Use a Multi-Task Model vs. Separate Models

1. When to use multi-task model:
- tasks are related and can benefit from shared knowledge, like Task 2's sentence categorization and sentiment analysis - both are sentence-level meaning mining
- one or few tasks have very limited data, multi-task enables shared learning across tasks
- 1 multi-task model is more computation efficient and much less inference-heavy than running separate models

2. when to se separate models:
- tasks are very different & having almost no shared knowledge there
- one or few task are complex that might require very different model architectures
- each task has plenty amount of labeled data to train with from scratch
- less of computation efficiency concern

## 3.3 How to deal with imbalance data between tasks:

If Task A has a lot of data while B has limited data, we can probably try:
- assign higher weights to task B's loss in the combined loss function, urging model to penalize more of B's misclassification 
- considering different sampling strategies: negative sampling A or oversampling B's data, or perform data augmentation for task B
- it could be a possibility that if task B has some open-source referrable related dataset, then can try to pretrain B's model on this related dataset first before fine tuning on the small dataset (basically means using transfer learning).

